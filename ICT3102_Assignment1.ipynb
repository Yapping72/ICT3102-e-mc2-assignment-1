{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yapping72/ICT3102-e-mc2-assignment-1/blob/main/ICT3102_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Project Dependencies`"
      ],
      "metadata": {
        "id": "RTEY9P0_foVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jA354BXgJEi",
        "outputId": "fe940368-9702-4cff-9456-f94d501eceda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Project Code`"
      ],
      "metadata": {
        "id": "IJfEacVcfsas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# byte_pair_tokenization = [\"openai-gpt\", \"gpt2\", \"NousResearch/Llama-2-13b-hf\"]\n",
        "# unigram_tokenization = [\"google/bigbird-roberta-base\", \"facebook/mbart-large-50-many-to-many-mmt\" , \"albert-base-v2\" , \"xlnet-base-cased\"]\n",
        "# wordpiece_tokenization = ['distilbert-base-uncased','google/mobilebert-uncased','funnel-transformer/small-base','sentence-transformers/all-mpnet-base-v2']\n",
        "# sentencepiece_tokenization = [\"google/flan-t5-base\"]\n",
        "\n",
        "corpus = [\"I have a new GPU!\", \"I wonder how fast the model will train on this.\", \"Car park there\"]\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceH4/self-instruct-seed\")\n",
        "# corpus = dataset['train']['instruction']\n",
        "\n",
        "def initialize_model(model_name:str):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  return tokenizer\n",
        "\n",
        "def time_model(tokenizer_object, corpus):\n",
        "    \"\"\"Calculate timing for encode_plus\"\"\"\n",
        "    # Capture the timeit result\n",
        "    total_time = 0\n",
        "    for text in corpus:\n",
        "        # Capture the timeit result for each text\n",
        "        timeit_result = %timeit -n 0 -r 1 -o tokenizer.encode_plus(text)\n",
        "        total_time += timeit_result.average\n",
        "    # Return the average time in milliseconds\n",
        "    return total_time * 1e3\n",
        "\n",
        "def time_model_batch(tokenizer, corpus):\n",
        "    \"\"\"Calculate timing for batch_encode_plus\"\"\"\n",
        "    # Capture the timeit result\n",
        "    timeit_result = %timeit -n 0 -r 1 -o tokenizer.batch_encode_plus(corpus)\n",
        "    # Return the average time in milliseconds\n",
        "    return timeit_result.average * 1e3\n",
        "\n",
        "def analyse_encode_plus(tokenizers: list, corpus: list) -> dict:\n",
        "    results = {}\n",
        "    results['method'] = \"Unbatched\"\n",
        "    for hugging_face_tokenizer in tokenizers:\n",
        "        try:\n",
        "          tokenizer = initialize_model(hugging_face_tokenizer)\n",
        "          formattedCorpus = tokenizer(corpus, truncation=True)\n",
        "          # print(formattedCorpus)\n",
        "          average_time = time_model(tokenizer, formattedCorpus)\n",
        "\n",
        "          # Extract tokenizer name or path for dictionary key\n",
        "          tokenizer_name = tokenizer.name_or_path\n",
        "          results[tokenizer_name] = average_time\n",
        "        except Exception as e:\n",
        "          print(f\"Error occured for {hugging_face_tokenizer}: {e}\")\n",
        "          continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyse_batch(tokenizers: list, corpus: list) -> dict:\n",
        "    results = {}\n",
        "    results['method'] = \"Batched\"\n",
        "    for hugging_face_tokenizer in tokenizers:\n",
        "        try:\n",
        "          tokenizer = initialize_model(hugging_face_tokenizer)\n",
        "          average_time = time_model_batch(tokenizer, corpus)\n",
        "\n",
        "          # Extract tokenizer name or path for dictionary key\n",
        "          tokenizer_name = tokenizer.name_or_path\n",
        "          results[tokenizer_name] = average_time\n",
        "        except Exception as e:\n",
        "          print(f\"Error occured for {hugging_face_tokenizer}: {e}\")\n",
        "          continue\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "NNPogCrMe_Z_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "byte_pair = [\"openai-gpt\", \"gpt2\", \"NousResearch/Llama-2-13b-hf\"]\n",
        "byte_pair_timing_unbatched = analyse_encode_plus(byte_pair, corpus)\n",
        "byte_pair_timing_batched = analyse_batch(byte_pair,corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Iia4ejimBET",
        "outputId": "ed343b51-0f74-4722-fcf8-a618b99f87e2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69.8 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10000 loops each)\n",
            "63.9 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10000 loops each)\n",
            "34.9 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10000 loops each)\n",
            "38 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10000 loops each)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36.4 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10000 loops each)\n",
            "67.4 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10000 loops each)\n",
            "197 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1000 loops each)\n",
            "262 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1000 loops each)\n",
            "224 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get response time for word_piece models\n",
        "word_piece =['distilbert-base-uncased','google/mobilebert-uncased','funnel-transformer/small-base','sentence-transformers/all-mpnet-base-v2']\n",
        "word_piece_timing_unbatched = analyse_encode_plus(word_piece, corpus)\n",
        "word_piece_timing_batched = analyse_batch(word_piece,corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkh2VVmlkCW5",
        "outputId": "bd6f9736-214e-42b8-c5f4-71e663788ce3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137 µs ± 33.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "101 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "237 µs ± 49.3 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "176 µs ± 17.8 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "189 µs ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "205 µs ± 18.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "193 µs ± 12 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "179 µs ± 11 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_piece = [\"google/flan-t5-base\"]\n",
        "sentence_piece_timing_unbatched = analyse_encode_plus(sentence_piece, corpus)\n",
        "sentence_piece_timing_batched = analyse_batch(sentence_piece,corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ6D8wBqkDMF",
        "outputId": "dabf0f79-0729-46dd-b321-cbebefc58692"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88.8 µs ± 26.5 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "156 µs ± 11 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigram = [\"google/bigbird-roberta-base\", \"facebook/mbart-large-50-many-to-many-mmt\" , \"albert-base-v2\" , \"xlnet-base-cased\"]\n",
        "unigram_timing_unbatched = analyse_encode_plus(unigram, corpus)\n",
        "unigram_timing_batched = analyse_batch(unigram,corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU-FtvtLkHDE",
        "outputId": "495037f5-2bbf-405e-da36-9a40ffdfd86f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79.3 µs ± 16.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "74.2 µs ± 1.01 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "132 µs ± 38.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "105 µs ± 26.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "143 µs ± 2.98 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "156 µs ± 4.26 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "200 µs ± 11.8 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "188 µs ± 12.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(byte_pair_timing_unbatched)\n",
        "print(byte_pair_timing_batched)\n",
        "print(word_piece_timing_unbatched)\n",
        "print(word_piece_timing_batched)\n",
        "print(sentence_piece_timing_unbatched)\n",
        "print(sentence_piece_timing_batched)\n",
        "print(unigram_timing_unbatched)\n",
        "print(unigram_timing_batched)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zy3oBn9kJlw",
        "outputId": "11fedc49-e5d2-4b74-cfe4-9ec1bc9fb9f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'method': 'Unbatched', 'openai-gpt': 0.13366224120002243, 'gpt2': 0.07297341069997855, 'NousResearch/Llama-2-13b-hf': 0.10379259730000286}\n",
            "{'method': 'Batched', 'openai-gpt': 0.19661517999998068, 'gpt2': 0.26245640100000855, 'NousResearch/Llama-2-13b-hf': 0.22394624000003205}\n"
          ]
        }
      ]
    }
  ]
}